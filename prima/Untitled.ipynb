{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.metrics import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from random import randint\n",
    "from sklearn import svm \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_dict = []\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "import os \n",
    "path = os.getcwd() +\"/ctgrs/\"\n",
    "for dirs,subdirs, files in os.walk(path):\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "       \n",
    "        file_path =  path + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        text = shakes.read()\n",
    "        word_tokens = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = list(map(lambda w: lemmatizer.lemmatize(w), word_tokens))\n",
    "        i=0\n",
    "        for l in lemmas :\n",
    "            i+=1\n",
    "            if i%4!=0:\n",
    "                train_data.append(l);\n",
    "                train_labels.append(file);\n",
    "            else:\n",
    "                test_data.append(l);\n",
    "                test_labels.append(file);\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "tfidf = TfidfVectorizer( stop_words='english')\n",
    "\n",
    "tfs_train = tfidf.fit_transform(train_data)\n",
    "\n",
    "tfs_test= tfidf.transform(test_data)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier_rbf = svm.SVC()\n",
    "t0 = time.time()\n",
    "classifier_rbf.fit(tfs_train, train_labels)\n",
    "t1 = time.time()\n",
    "prediction_rbf = classifier_rbf.predict(tfs_test)\n",
    "t2 = time.time()\n",
    "time_rbf_train = t1-t0\n",
    "time_rbf_predict = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(tfs_train, train_labels)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(tfs_test)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVC(kernel=rbf)\n",
      "Training time: 0.049056s; Prediction time: 0.020564s\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   alcohol.txt       1.00      1.00      1.00         1\n",
      "   animals.txt       1.00      1.00      1.00         1\n",
      "bad_habits.txt       1.00      1.00      1.00         1\n",
      " education.txt       1.00      1.00      1.00         1\n",
      "\n",
      "   avg / total       1.00      1.00      1.00         4\n",
      "\n",
      "Results for SVC(kernel=linear)\n",
      "Training time: 0.016795s; Prediction time: 0.011727s\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "   alcohol.txt       1.00      1.00      1.00         1\n",
      "   animals.txt       1.00      1.00      1.00         1\n",
      "bad_habits.txt       1.00      1.00      1.00         1\n",
      " education.txt       1.00      1.00      1.00         1\n",
      "\n",
      "   avg / total       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Results for SVC(kernel=rbf)\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_rbf_train, time_rbf_predict))\n",
    "print(classification_report(test_labels, prediction_rbf))\n",
    "print(\"Results for SVC(kernel=linear)\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "print(classification_report(test_labels, prediction_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "#BetterEasterTraditions https:â€¦\n",
      " maybe I should get another cat and name it rick\n",
      " looking back i think the pet shop fire scene in pee wee's big adventure affected my identity more than anything else\n",
      " 4.i think these are \"hush puppies\" idk and idc I just know they are horrid\n",
      " Guess who has one eye and has to attend racial sensitivity training for his sexy bunny costume?\n",
      "Total 5670802 words\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print (\"Example:\\n\\n%s\"%sentences[1])\n",
    "\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "print (\"Total %d words\"%len(word_tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "print (\"Length of stop_words %d\"%len(stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_word_tokens = [w for w in word_tokens if not w in stop]\n",
    "print (\"Total %d words without stop-words\"%len(filtered_word_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def removePunctuation(text):\n",
    "    p = re.compile('[^a-zA-Z0-9_ ]')\n",
    "    return p.sub('', text.lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = removePunctuation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stems = map(lambda w: porter_stemmer.stem(w), filtered_word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Total %d after stemming\"%len(list(set(stems))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmas = list(map(lambda w: lemmatizer.lemmatize(w), filtered_word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Total %d after lemmatization\"%len(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(text.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
